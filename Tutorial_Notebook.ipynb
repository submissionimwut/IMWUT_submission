{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "base_directory = \".\"\n",
    "repo = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install numpy scipy scikit-learn seaborn matplotlib pandas lightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import random\n",
    "import glob\n",
    "from datetime import date\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "# progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Various metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# Setting seeds\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(base_directory, exist_ok=True)\n",
    "%cd $base_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accelerometer_timeseries(timeseries, title=\"\", figsize=(15,6), xlim=None, ylim=None):\n",
    "    plt.figure(figsize=figsize)\n",
    "    if len(timeseries.shape) == 1:\n",
    "        plt.plot(timeseries)\n",
    "    else:\n",
    "        for i in range(3):\n",
    "            plt.plot(timeseries[:, i], label=[\"x\",\"y\",\"z\"][i])\n",
    "    if xlim is None:\n",
    "        plt.xlim(0, len(timeseries))\n",
    "    else:\n",
    "        plt.xlim(*xlim)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "label_mapping = {\n",
    "    0: \"Downstairs\",\n",
    "    1: \"Upstairs\",\n",
    "    2: \"Walking\",\n",
    "    3: \"Jogging\",\n",
    "    4: \"Standing\",\n",
    "    5: \"Sitting\",\n",
    "}\n",
    "num_ticks = 6\n",
    "cm_ticks = np.linspace(0, num_ticks-1, num_ticks, dtype=int)\n",
    "cm_ticklabels = [label_mapping[idx] for idx in cm_ticks]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 **Downloading the Motionsense data**\n",
    "\n",
    "Already available in the Github repo! So, there is not need to download!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_preparation.prepare_motionsense as prepare\n",
    "\n",
    "# Loading the arguments first\n",
    "args = prepare.load_args()\n",
    "print(args)\n",
    "\n",
    "# Obtaining the processed data\n",
    "processed = prepare.prepare_data(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 6))\n",
    "for i, split, title in zip(range(3), ['train', 'val', 'test'], [\"Train\", \"Val\", \"Test\"]):\n",
    "    labels, counts = np.unique(processed[split]['labels'], return_counts=True)\n",
    "    axs[i].bar(labels, counts)\n",
    "    axs[i].set_title(title + \" set\")\n",
    "    axs[i].set_xticks(range(6), range(6))\n",
    "    # axs[i].set_ylim([0, 65000])\n",
    "axs[0].set_ylabel(\"Count\")\n",
    "axs[1].set_xlabel(\"Classes\")\n",
    "# plt.ylim([0, 65000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accelerometer_timeseries(processed['train']['data'][:100], f\"Training accelerometer trace - Activity {processed['train']['labels'][100]}\", figsize=(10,4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Activity Recognition Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 **Segmentation: obtaining windows of sensor data through sliding window**\n",
    "\n",
    "### Goal: take stream of sensor data and return windows + labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_files = glob.glob(os.path.join(repo, \"data_preparation\", \"all_data\", \"*\", \"motionsense.pkl\"))\n",
    "processed_data_files.sort(key=os.path.getmtime)\n",
    "processed_data_file = processed_data_files[-1]\n",
    "processed = pd.read_pickle(processed_data_file)\n",
    "processed_data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ecdf.extract_ecdf_train_classifier as ecdf\n",
    "\n",
    "# Obtaining the segmented data\n",
    "segmented_data = ecdf.generate_windowed_data(processed=processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accelerometer_timeseries(processed['train']['data'][:150], f\"Before segmentation - Activity {processed['train']['labels'][150]}\", ylim=(-3,2), figsize=(10,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    plot_accelerometer_timeseries(segmented_data['train']['data'][i], f\"After segmentation - Segment {i} - Activity {segmented_data['train']['labels'][i]}\", xlim=(-i*50, 150-i*50), ylim=(-3,2), figsize=(10,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 **Extracting features: ECDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the ECDF features\n",
    "ecdf_features = ecdf.compute_ecdf_features(segmented_data=segmented_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 **Training a Random Forest classifier with ECDF features for Activity Recognition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the RF classifier\n",
    "trained_classifier, log_ecdf = ecdf.train_rf_classifier(ecdf=ecdf_features, segmented_data=segmented_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_ecdf = confusion_matrix(segmented_data[\"test\"][\"labels\"], trained_classifier.predict(ecdf_features[\"test\"]))\n",
    "confusion_matrix_ecdf_norm = confusion_matrix_ecdf / np.sum(confusion_matrix_ecdf, axis=1, keepdims=True)\n",
    "os.makedirs(os.path.join(repo, \"ecdf\", \"saved_logs\", \"current\"), exist_ok=True)\n",
    "with open(os.path.join(repo, \"ecdf\", \"saved_logs\", \"current\", \"ecdf_eval_log.pkl\"), 'wb') as f:\n",
    "  pickle.dump(\n",
    "      {\"cm\": confusion_matrix_ecdf_norm,\n",
    "       \"f1\": log_ecdf\n",
    "      },\n",
    "  f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(confusion_matrix_ecdf_norm, annot=True, fmt='.1%', cmap='Blues', annot_kws={\"fontsize\":8}, yticklabels=cm_ticklabels, xticklabels=cm_ticklabels)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Convolutional Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conv_classifier import evaluate_with_classifier\n",
    "from conv_classifier import model\n",
    "from conv_classifier import arguments_dict\n",
    "from conv_classifier import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = arguments_dict.load_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_files = glob.glob(os.path.join(repo, \"data_preparation\", \"all_data\", \"*\", \"motionsense.pkl\"))\n",
    "processed_data_files.sort(key=os.path.getmtime)\n",
    "processed_data_file = processed_data_files[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['root_dir'] = os.path.split(processed_data_file)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.Classifier(args=args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.set_all_seeds(args['random_seed'])\n",
    "args['num_epochs'] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_with_classifier(args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we load the logs\n",
    "log_files = glob.glob(os.path.join(repo, \"conv_classifier\", \"saved_logs\", \"*\", \"classifier*_log.pkl\"))\n",
    "log_files.sort(key=os.path.getmtime)\n",
    "log_file = log_files[-1]\n",
    "logs_simclr = pd.read_pickle(log_file)\n",
    "\n",
    "\n",
    "plt.figure(dpi=200, figsize=(8, 4))\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.arange(len(logs_simclr.loss['train'])), logs_simclr.loss['train'], label='Train loss')\n",
    "plt.plot(np.arange(len(logs_simclr.loss['val'])), logs_simclr.loss['val'], label='Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.arange(len(logs_simclr.f1_score['train'])), logs_simclr.f1_score['train'], label='Train F1-score')\n",
    "plt.plot(np.arange(len(logs_simclr.f1_score['val'])), logs_simclr.f1_score['val'], label='Validation F1-score')\n",
    "plt.plot(np.arange(len(logs_simclr.f1_score['test'])), logs_simclr.f1_score['test'], label='Test F1-score')\n",
    "\n",
    "# plotting the best val F1-score\n",
    "plt.plot(logs_simclr.best_meter.epoch, logs_simclr.best_meter.f1_score['val'], 'rx')\n",
    "plt.plot(logs_simclr.best_meter.epoch, logs_simclr.best_meter.f1_score['test'], 'r+')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1-score')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 SimCLR for learning representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Import & Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simclr import arguments_dict\n",
    "from simclr import utils\n",
    "from simclr import pretrainer\n",
    "from simclr import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = arguments_dict.load_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating the location of the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_files = glob.glob(os.path.join(repo, \"data_preparation\", \"all_data\", \"*\", \"motionsense.pkl\"))\n",
    "processed_data_files.sort(key=os.path.getmtime)\n",
    "processed_data_file = processed_data_files[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['root_dir'] = os.path.split(processed_data_file)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Pre-training using SimCLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the seeds for pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.set_all_seeds(args['random_seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us print the SimCLR model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simclr_model = model.SimCLR(args=args)\n",
    "print(simclr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the pre-training using the SimCLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrainer.learn_model(args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the loss values to see the trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we load the logs\n",
    "log_files = glob.glob(os.path.join(repo, \"simclr\", \"saved_logs\", \"*\", \"simclr_*.pkl\"))\n",
    "log_files.sort(key=os.path.getmtime)\n",
    "log_files = list(filter(lambda x: not x.endswith(\"_eval_log.pkl\"), log_files))\n",
    "log_file = log_files[-1]\n",
    "logs = pd.read_pickle(log_file)\n",
    "\n",
    "plt.figure(dpi=200)\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.plot(np.arange(len(logs.loss['train'])), logs.loss['train'], label='Train loss')\n",
    "plt.plot(np.arange(len(logs.loss['val'])), logs.loss['val'], label='Validation loss')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Classification with the learned features.\n",
    "\n",
    "For that, we first set the location of the trained model, so we can load the learned weights  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simclr import evaluate_with_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.Classifier(args=args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_folders = glob.glob(os.path.join(repo, \"simclr\", \"saved_weights\", \"*\"))\n",
    "saved_model_folders.sort(key=os.path.getmtime)\n",
    "saved_model_folder = saved_model_folders[-1]\n",
    "\n",
    "args['saved_model_folder'] = saved_model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_with_classifier(args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can now plot the loss and f1-scores to see how performance improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we load the logs\n",
    "log_files = glob.glob(os.path.join(repo, \"simclr\", \"saved_logs\", \"*\", \"simclr*_eval_log.pkl\"))\n",
    "log_files.sort(key=os.path.getmtime)\n",
    "log_file = log_files[-1]\n",
    "logs_simclr = pd.read_pickle(log_file)\n",
    "\n",
    "\n",
    "plt.figure(dpi=200, figsize=(8, 4))\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.arange(len(logs_simclr.loss['train'])), logs_simclr.loss['train'], label='Train loss')\n",
    "plt.plot(np.arange(len(logs_simclr.loss['val'])), logs_simclr.loss['val'], label='Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.arange(len(logs_simclr.f1_score['train'])), logs_simclr.f1_score['train'], label='Train F1-score')\n",
    "plt.plot(np.arange(len(logs_simclr.f1_score['val'])), logs_simclr.f1_score['val'], label='Validation F1-score')\n",
    "plt.plot(np.arange(len(logs_simclr.f1_score['test'])), logs_simclr.f1_score['test'], label='Test F1-score')\n",
    "\n",
    "# plotting the best val F1-score\n",
    "plt.plot(logs_simclr.best_meter.epoch, logs_simclr.best_meter.f1_score['val'], 'rx')\n",
    "plt.plot(logs_simclr.best_meter.epoch, logs_simclr.best_meter.f1_score['test'], 'r+')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1-score')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the confusion matrix to see where the classes get confused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(logs_simclr.best_meter.confusion_matrix['test'], annot=True, fmt='.1%', cmap='Blues', annot_kws={\"fontsize\":8}, yticklabels=cm_ticklabels, xticklabels=cm_ticklabels)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Result comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(repo, \"ecdf\", \"saved_logs\", \"current\", \"ecdf_eval_log.pkl\"), 'rb') as f:\n",
    "    ecdf_obj = pickle.load(f)\n",
    "    confusion_matrix_ecdf_norm = ecdf_obj[\"cm\"]\n",
    "    f1_ecdf_test = ecdf_obj[\"f1\"][\"test\"]\n",
    "\n",
    "log_files = glob.glob(os.path.join(repo, \"conv_classifier\", \"saved_logs\", \"*\", \"classifier*_eval_log.pkl\"))\n",
    "log_files.sort(key=os.path.getmtime)\n",
    "log_file = log_files[-1]\n",
    "logs_conv_classifier = pd.read_pickle(log_file)\n",
    "\n",
    "log_files = glob.glob(os.path.join(repo, \"simclr\", \"saved_logs\", \"*\", \"simclr*_eval_log.pkl\"))\n",
    "log_files.sort(key=os.path.getmtime)\n",
    "log_file = log_files[-1]\n",
    "logs_simclr = pd.read_pickle(log_file)\n",
    "\n",
    "confusion_matrices = {\n",
    "    \"ECDF\": confusion_matrix_ecdf_norm,\n",
    "    \"DeepConvLSTM\": logs_conv_classifier.best_meter.confusion_matrix['test'],\n",
    "    \"SimCLR\": logs_simclr.best_meter.confusion_matrix['test']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(\n",
    "    [\"ECDF\", \"Conv Classifier\", \"SimCLR\"],\n",
    "    [\n",
    "        f1_ecdf_test,\n",
    "        logs_conv_classifier.best_meter.f1_score['test'],\n",
    "        logs_simclr.best_meter.f1_score['test']\n",
    "    ],\n",
    "    color=['tab:blue', 'tab:red', 'tab:green']\n",
    ")\n",
    "plt.ylim([0.5, 1.0])\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.xlabel(\"Training Method\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=3, figsize=(20, 6))\n",
    "for i, method in enumerate(confusion_matrices):\n",
    "    sns.heatmap(\n",
    "        confusion_matrices[method],\n",
    "        ax=axs[i],\n",
    "        annot=True, fmt='.1%', cmap='Blues', annot_kws={\"fontsize\":8},\n",
    "        vmin=0, vmax=1, cbar=False,\n",
    "        yticklabels=cm_ticklabels, xticklabels=cm_ticklabels\n",
    "    )\n",
    "    axs[i].set_xticks(axs[i].get_xticks())\n",
    "    axs[i].set_xticklabels(axs[i].get_xticklabels(), rotation=45, ha='right')\n",
    "    # axs[i].tick_params(axis='both', which='major', labelsize=6)\n",
    "    axs[i].set_title(method)\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
